{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lxper_mission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5kDidA3GSsmZZxoBTgRLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssifood/CoLA_backend/blob/main/lxper_mission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaTjIH4RaqQ7"
      },
      "source": [
        "머신러닝 nlp엔지니어 과제\n",
        "huggingface transformers 기반 모델과 fastapi 라이브러리로 다음의 기능 수행하는 단순 api 웹서버 구현\n",
        "\n",
        "a.요건\n",
        "\n",
        "    GLUE task중 CoLA 데이터를 활용해, 주어진 문장이 문법적으로 적합한지를 판별하는 API를 개발\n",
        "    -최소한 3개의 huggingface transformers에서 제공하는 prtrained model에 GLU CoLa데이터셋을 입력해 finetunning하고, 이중 BEST모형을 선택한다.\n",
        "    -선택된 모형을 fastapi 라이브러리를 활용해 서비스하고,서비스 포트는 8000번을 활용\n",
        "    -/generate 엔드포인트는 json payload를 HTTP POST 방식으로 이벽받아 JSON response를 반환하여야 한다.\n",
        "\n",
        "        -입력값:{\"passage\":\"I are a boy\"}\n",
        "            -passage라는 키값을 지니며, 이 키의 값은 문자열로, 문법적합성을 판정할 문장이다.\n",
        "        -출력값: {\"prob\":0.001}\n",
        "            -prob라는 키를 가지며, 이 키의 값은 실수로 ,문법적합성을  나타내는 값이다. 1에 가까울 수록 문법적으로 적합함을 나타낸다.\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQmOUX03ak5S",
        "outputId": "bc3c9055-dba5-4c99-9154-5a39e031681c"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 53780, done.\u001b[K\n",
            "remote: Total 53780 (delta 0), reused 0 (delta 0), pack-reused 53780\u001b[K\n",
            "Receiving objects: 100% (53780/53780), 40.23 MiB | 30.29 MiB/s, done.\n",
            "Resolving deltas: 100% (37521/37521), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOi8-E05etj4",
        "outputId": "3476c6f1-9514-4d88-dc5d-da785ad73c80"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 34.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 37.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 28.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 27.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 29.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 23.6MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 24.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 25.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 24.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 24.1MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 24.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 24.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 24.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 24.1MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 24.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 24.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 24.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 24.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3a8d5b52b9fd0175b09d24e53ef597bfdb5809de14ce68156b230932af794c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 197kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNtU53SzeYIj",
        "outputId": "b466e687-28bc-43ce-eaa6-95ef0cebbcd2"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('glue','cola')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd-8GGrFfJZm",
        "outputId": "fc77f5b8-f60d-48da-d59b-ad60afb647d2"
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 8551\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 1043\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 1063\n",
            "    })\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjcYcBmihA0Z",
        "outputId": "b134f346-e4a3-4a1c-ca68-5dc17eab9852"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers/examples/text-classification\")\n",
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md    run_pl_glue.py  run_tf_glue.py\t\t    run_xnli.py\n",
            "run_glue.py  run_pl.sh\t     run_tf_text_classification.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNE1fUXFfObz"
      },
      "source": [
        "!export TASK_NAME=COLA"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A62igrAhnkx6",
        "outputId": "ad430d98-52d6-4cd6-8333-f70e1d5f7b29"
      },
      "source": [
        "!echo $TASK_NAME\n",
        "!echo TASK_NAME\n",
        "#이거왜 리눅스 전역변수가안되냐?? 그래서 $TASK_NAME 수동으로 입력"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TASK_NAME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPzEVQSnipPA",
        "outputId": "db0e8f9d-cdf0-4084-8930-4f91027ffac9"
      },
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path bert-base-cased \\\n",
        "  --task_name cola \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --output_dir /tmp/cola/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-30 00:59:49.869873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "11/30/2020 00:59:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/30/2020 00:59:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/cola/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov30_00-59-51_c72c417f1f67', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/cola/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
            "11/30/2020 00:59:51 - INFO - filelock -   Lock 140075025847184 acquired on /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 00:59:51,743 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmppr_gvw0d\n",
            "Downloading: 100% 433/433 [00:00<00:00, 546kB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 00:59:51,819 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|file_utils.py:1169] 2020-11-30 00:59:51,819 >> creating metadata file for /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "11/30/2020 00:59:51 - INFO - filelock -   Lock 140075025847184 released on /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655.lock\n",
            "[INFO|configuration_utils.py:413] 2020-11-30 00:59:51,820 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|configuration_utils.py:449] 2020-11-30 00:59:51,821 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:413] 2020-11-30 00:59:51,905 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|configuration_utils.py:449] 2020-11-30 00:59:51,905 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "11/30/2020 00:59:51 - INFO - filelock -   Lock 140075018655728 acquired on /root/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 00:59:51,990 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp56uc3hx5\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 3.18MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 00:59:52,142 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1169] 2020-11-30 00:59:52,142 >> creating metadata file for /root/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "11/30/2020 00:59:52 - INFO - filelock -   Lock 140075018655728 released on /root/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
            "11/30/2020 00:59:52 - INFO - filelock -   Lock 140075025847184 acquired on /root/.cache/torch/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 00:59:52,233 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpf1tmarye\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 6.11MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 00:59:52,396 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/torch/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1169] 2020-11-30 00:59:52,396 >> creating metadata file for /root/.cache/torch/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "11/30/2020 00:59:52 - INFO - filelock -   Lock 140075025847184 released on /root/.cache/torch/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
            "[INFO|tokenization_utils_base.py:1650] 2020-11-30 00:59:52,396 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1650] 2020-11-30 00:59:52,396 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/torch/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "11/30/2020 00:59:52 - INFO - filelock -   Lock 140075025847184 acquired on /root/.cache/torch/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 00:59:52,497 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnlx2ehrk\n",
            "Downloading: 100% 436M/436M [00:06<00:00, 64.8MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 00:59:59,351 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[INFO|file_utils.py:1169] 2020-11-30 00:59:59,351 >> creating metadata file for /root/.cache/torch/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "11/30/2020 00:59:59 - INFO - filelock -   Lock 140075025847184 released on /root/.cache/torch/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda.lock\n",
            "[INFO|modeling_utils.py:940] 2020-11-30 00:59:59,351 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
            "[WARNING|modeling_utils.py:1048] 2020-11-30 01:00:02,911 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1059] 2020-11-30 01:00:02,911 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 9/9 [00:00<00:00, 20.57ba/s]\n",
            "100% 2/2 [00:00<00:00, 34.87ba/s]\n",
            "100% 2/2 [00:00<00:00, 29.83ba/s]\n",
            "11/30/2020 01:00:03 - INFO - __main__ -   Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1824, 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence': 'I acknowledged that my father, he was tight as an owl.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/30/2020 01:00:03 - INFO - __main__ -   Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 409, 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'For him to do that would be a mistake.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "11/30/2020 01:00:03 - INFO - __main__ -   Sample 4506 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4506, 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': 'Mary sang a song, but Lee never did.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading: 4.39kB [00:00, 5.83MB/s]       \n",
            "[INFO|trainer.py:388] 2020-11-30 01:00:17,797 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
            "[INFO|trainer.py:388] 2020-11-30 01:00:17,797 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
            "[INFO|trainer.py:693] 2020-11-30 01:00:17,801 >> ***** Running training *****\n",
            "[INFO|trainer.py:694] 2020-11-30 01:00:17,801 >>   Num examples = 8551\n",
            "[INFO|trainer.py:695] 2020-11-30 01:00:17,801 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:696] 2020-11-30 01:00:17,801 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:697] 2020-11-30 01:00:17,801 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:698] 2020-11-30 01:00:17,801 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:699] 2020-11-30 01:00:17,801 >>   Total optimization steps = 804\n",
            "{'loss': 0.405658447265625, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.8656716417910446}\n",
            " 62% 500/804 [05:31<03:33,  1.43it/s][INFO|trainer.py:1222] 2020-11-30 01:05:49,444 >> Saving model checkpoint to /tmp/cola/checkpoint-500\n",
            "[INFO|configuration_utils.py:282] 2020-11-30 01:05:49,446 >> Configuration saved in /tmp/cola/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:740] 2020-11-30 01:05:50,997 >> Model weights saved in /tmp/cola/checkpoint-500/pytorch_model.bin\n",
            "100% 804/804 [09:07<00:00,  1.83it/s][INFO|trainer.py:829] 2020-11-30 01:09:25,236 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 3.0}\n",
            "100% 804/804 [09:07<00:00,  1.47it/s]\n",
            "[INFO|trainer.py:1222] 2020-11-30 01:09:25,238 >> Saving model checkpoint to /tmp/cola/\n",
            "[INFO|configuration_utils.py:282] 2020-11-30 01:09:25,240 >> Configuration saved in /tmp/cola/config.json\n",
            "[INFO|modeling_utils.py:740] 2020-11-30 01:09:26,842 >> Model weights saved in /tmp/cola/pytorch_model.bin\n",
            "11/30/2020 01:09:26 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:388] 2020-11-30 01:09:26,875 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx.\n",
            "[INFO|trainer.py:1387] 2020-11-30 01:09:26,875 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1388] 2020-11-30 01:09:26,876 >>   Num examples = 1043\n",
            "[INFO|trainer.py:1389] 2020-11-30 01:09:26,876 >>   Batch size = 8\n",
            "100% 131/131 [00:08<00:00, 14.17it/s]11/30/2020 01:09:35 - INFO - /usr/local/lib/python3.6/dist-packages/datasets/metric.py -   Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "100% 131/131 [00:08<00:00, 14.59it/s]\n",
            "11/30/2020 01:09:35 - INFO - __main__ -   ***** Eval results cola *****\n",
            "11/30/2020 01:09:35 - INFO - __main__ -     eval_loss = 0.5114678144454956\n",
            "11/30/2020 01:09:35 - INFO - __main__ -     eval_matthews_correlation = 0.5981951351808346\n",
            "11/30/2020 01:09:35 - INFO - __main__ -     epoch = 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fwXALdDg5xp",
        "outputId": "ad6dfe18-4b25-4a5f-9112-148565a50229"
      },
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path xlm-roberta-base \\\n",
        "  --task_name cola \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --output_dir /tmp/cola_xlm-roberta-base/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-30 01:20:40.216112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "11/30/2020 01:20:41 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/30/2020 01:20:41 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/cola_xlm-roberta-base/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov30_01-20-41_c72c417f1f67', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/cola_xlm-roberta-base/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
            "11/30/2020 01:20:42 - INFO - filelock -   Lock 140645003448896 acquired on /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 01:20:42,187 >> https://huggingface.co/xlm-roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpb06iz28q\n",
            "Downloading: 100% 512/512 [00:00<00:00, 702kB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 01:20:42,266 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/config.json in cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|file_utils.py:1169] 2020-11-30 01:20:42,266 >> creating metadata file for /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "11/30/2020 01:20:42 - INFO - filelock -   Lock 140645003448896 released on /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6.lock\n",
            "[INFO|configuration_utils.py:413] 2020-11-30 01:20:42,267 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:449] 2020-11-30 01:20:42,268 >> Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:413] 2020-11-30 01:20:42,341 >> loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n",
            "[INFO|configuration_utils.py:449] 2020-11-30 01:20:42,341 >> Model config XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "11/30/2020 01:20:42 - INFO - filelock -   Lock 140645003277760 acquired on /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 01:20:42,511 >> https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp37np48_3\n",
            "Downloading: 100% 5.07M/5.07M [00:00<00:00, 30.5MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 01:20:42,806 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|file_utils.py:1169] 2020-11-30 01:20:42,806 >> creating metadata file for /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "11/30/2020 01:20:42 - INFO - filelock -   Lock 140645003277760 released on /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8.lock\n",
            "11/30/2020 01:20:43 - INFO - filelock -   Lock 140647814747248 acquired on /root/.cache/torch/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 01:20:43,015 >> https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4n6ci_w3\n",
            "Downloading: 100% 9.10M/9.10M [00:00<00:00, 37.4MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 01:20:43,412 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/torch/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "[INFO|file_utils.py:1169] 2020-11-30 01:20:43,412 >> creating metadata file for /root/.cache/torch/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "11/30/2020 01:20:43 - INFO - filelock -   Lock 140647814747248 released on /root/.cache/torch/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7.lock\n",
            "[INFO|tokenization_utils_base.py:1650] 2020-11-30 01:20:43,413 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "[INFO|tokenization_utils_base.py:1650] 2020-11-30 01:20:43,413 >> loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/torch/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
            "11/30/2020 01:20:44 - INFO - filelock -   Lock 140647814747248 acquired on /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2.lock\n",
            "[INFO|file_utils.py:1162] 2020-11-30 01:20:44,015 >> https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpyz_q4okr\n",
            "Downloading: 100% 1.12G/1.12G [00:24<00:00, 45.1MB/s]\n",
            "[INFO|file_utils.py:1166] 2020-11-30 01:21:08,879 >> storing https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[INFO|file_utils.py:1169] 2020-11-30 01:21:08,879 >> creating metadata file for /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "11/30/2020 01:21:08 - INFO - filelock -   Lock 140647814747248 released on /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2.lock\n",
            "[INFO|modeling_utils.py:940] 2020-11-30 01:21:08,879 >> loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
            "[WARNING|modeling_utils.py:1048] 2020-11-30 01:21:17,921 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1059] 2020-11-30 01:21:17,921 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 9/9 [00:00<00:00, 17.62ba/s]\n",
            "100% 2/2 [00:00<00:00, 33.61ba/s]\n",
            "100% 2/2 [00:00<00:00, 33.55ba/s]\n",
            "11/30/2020 01:21:18 - INFO - __main__ -   Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1824, 'input_ids': [0, 87, 224344, 71, 450, 759, 67373, 4, 764, 509, 107137, 237, 142, 36, 27751, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': 'I acknowledged that my father, he was tight as an owl.'}.\n",
            "11/30/2020 01:21:18 - INFO - __main__ -   Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 409, 'input_ids': [0, 1326, 4049, 47, 54, 450, 2806, 186, 10, 121742, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'For him to do that would be a mistake.'}.\n",
            "11/30/2020 01:21:18 - INFO - __main__ -   Sample 4506 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4506, 'input_ids': [0, 23213, 6079, 10, 11531, 4, 1284, 19824, 8306, 6777, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'Mary sang a song, but Lee never did.'}.\n",
            "[INFO|trainer.py:388] 2020-11-30 01:21:31,914 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:388] 2020-11-30 01:21:31,915 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: idx, sentence.\n",
            "[INFO|trainer.py:693] 2020-11-30 01:21:31,919 >> ***** Running training *****\n",
            "[INFO|trainer.py:694] 2020-11-30 01:21:31,919 >>   Num examples = 8551\n",
            "[INFO|trainer.py:695] 2020-11-30 01:21:31,919 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:696] 2020-11-30 01:21:31,919 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:697] 2020-11-30 01:21:31,919 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:698] 2020-11-30 01:21:31,919 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:699] 2020-11-30 01:21:31,920 >>   Total optimization steps = 804\n",
            "{'loss': 0.571120849609375, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.8656716417910446}\n",
            " 62% 500/804 [06:04<03:52,  1.31it/s][INFO|trainer.py:1222] 2020-11-30 01:27:36,638 >> Saving model checkpoint to /tmp/cola_xlm-roberta-base/checkpoint-500\n",
            "[INFO|configuration_utils.py:282] 2020-11-30 01:27:36,639 >> Configuration saved in /tmp/cola_xlm-roberta-base/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:740] 2020-11-30 01:27:42,277 >> Model weights saved in /tmp/cola_xlm-roberta-base/checkpoint-500/pytorch_model.bin\n",
            " 85% 687/804 [09:02<01:29,  1.31it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cVA46S5edM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}